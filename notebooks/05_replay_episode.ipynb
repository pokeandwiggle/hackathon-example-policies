{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcf2d29",
   "metadata": {},
   "source": [
    "# üîÑ Replay Episode to Robot\n",
    "\n",
    "This notebook replays **ground truth actions** from a recorded episode directly to the robot.\n",
    "\n",
    "No trained policy is needed - this is useful for:\n",
    "1. Verifying that recorded demonstrations are valid\n",
    "2. Testing robot connectivity and action execution\n",
    "3. Debugging the action space and robot interface\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This will move the physical robot! Ensure the workspace is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cbf25",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set paths and replay parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dataset path - try multiple possible locations\n",
    "possible_dataset_paths = [\n",
    "    Path(\"/data/lerobot/[TODO]\"),  # JupyterHub absolute\n",
    "]\n",
    "\n",
    "DATASET_DIR = None\n",
    "for p in possible_dataset_paths:\n",
    "    if p.exists():\n",
    "        DATASET_DIR = p\n",
    "        print(f\"‚úÖ Found dataset at: {p}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"‚ùå Not found: {p}\")\n",
    "\n",
    "if DATASET_DIR is None:\n",
    "    print(\"\\n‚ö†Ô∏è Please set DATASET_DIR manually\")\n",
    "\n",
    "# TODO: Set robot IP address\n",
    "SERVER_ENDPOINT = \"<robot_ip_address>:50051\"\n",
    "\n",
    "# Episode to replay (0-indexed)\n",
    "EPISODE_INDEX = 0\n",
    "\n",
    "# Replay frequency in Hz - should match the recording FPS\n",
    "REPLAY_FREQUENCY_HZ = 10.0\n",
    "\n",
    "# Speed multiplier (1.0 = normal speed, 0.5 = half speed, 2.0 = double speed)\n",
    "SPEED_MULTIPLIER = 1.0\n",
    "\n",
    "print(f\"\\nDataset: {DATASET_DIR}\")\n",
    "print(f\"Episode: {EPISODE_INDEX}\")\n",
    "print(f\"Robot server: {SERVER_ENDPOINT}\")\n",
    "print(f\"Replay frequency: {REPLAY_FREQUENCY_HZ} Hz\")\n",
    "print(f\"Speed multiplier: {SPEED_MULTIPLIER}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d09455",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Metadata\n",
    "\n",
    "Load the dataset using LeRobotDataset and extract metadata for the action translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc35ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "from example_policies.robot_deploy.deploy_core.policy_loader import load_metadata\n",
    "from example_policies.robot_deploy.deploy_core.action_translator import ActionTranslator\n",
    "from example_policies.utils.constants import ACTION, OBSERVATION_STATE\n",
    "from example_policies.utils.action_order import ActionMode\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load metadata from dataset\n",
    "metadata = load_metadata(DATASET_DIR)\n",
    "print(f\"‚úÖ Metadata loaded\")\n",
    "\n",
    "# Create a fake config that mimics what the policy loader would provide\n",
    "class FakeConfig:\n",
    "    def __init__(self, m):\n",
    "        self.metadata = m\n",
    "        self.device = device\n",
    "        self.output_features = {}\n",
    "        self.input_features = {}\n",
    "        self.input_features[OBSERVATION_STATE] = np.asarray(\n",
    "            m[\"features\"][OBSERVATION_STATE][\"names\"]\n",
    "        )\n",
    "        self.output_features[ACTION] = np.asarray(m[\"features\"][ACTION][\"names\"])\n",
    "\n",
    "    def get_tcp_from_state(self, state: np.ndarray) -> np.ndarray:\n",
    "        state_names = []\n",
    "        state_names.extend([f\"tcp_left_pos_{i}\" for i in \"xyz\"])\n",
    "        state_names.extend([f\"tcp_left_quat_{i}\" for i in \"xyzw\"])\n",
    "        state_names.extend([f\"tcp_right_pos_{i}\" for i in \"xyz\"])\n",
    "        state_names.extend([f\"tcp_right_quat_{i}\" for i in \"xyzw\"])\n",
    "\n",
    "        state_indices = [\n",
    "            np.where(self.input_features[OBSERVATION_STATE] == name)[0][0]\n",
    "            for name in state_names\n",
    "        ]\n",
    "        return state[:, state_indices]\n",
    "\n",
    "cfg = FakeConfig(metadata)\n",
    "print(f\"Input features: {cfg.input_features[OBSERVATION_STATE][:5]}...\")\n",
    "print(f\"Output features: {cfg.output_features[ACTION][:5]}...\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = LeRobotDataset(\n",
    "    repo_id=DATASET_DIR.name,\n",
    "    root=DATASET_DIR,\n",
    "    episodes=[EPISODE_INDEX],\n",
    ")\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} frames\")\n",
    "\n",
    "# Create dataloader with num_workers=0 to avoid memory issues\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,  # Use 0 workers to avoid memory issues\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "print(f\"‚úÖ Dataloader created\")\n",
    "\n",
    "# Create action translator\n",
    "action_translator = ActionTranslator(cfg)\n",
    "print(f\"‚úÖ Action translator created\")\n",
    "print(f\"   Action mode: {action_translator.action_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb8d17",
   "metadata": {},
   "source": [
    "## 3. Visualize Actions (Optional)\n",
    "\n",
    "Preview the ground truth actions before sending to the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load actions directly from parquet (avoids loading videos via LeRobotDataset)\n",
    "parquet_path = DATASET_DIR / f\"data/chunk-000/episode_{EPISODE_INDEX:06d}.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Extract action columns\n",
    "action_cols = sorted([c for c in df.columns if c.startswith(\"action\")])\n",
    "\n",
    "# Extract actions as numpy array\n",
    "actions_list = []\n",
    "for idx in range(len(df)):\n",
    "    row = df.iloc[idx]\n",
    "    action_values = []\n",
    "    for c in action_cols:\n",
    "        val = row[c]\n",
    "        if isinstance(val, np.ndarray):\n",
    "            action_values.extend(val.flatten().tolist())\n",
    "        else:\n",
    "            action_values.append(float(val))\n",
    "    actions_list.append(action_values)\n",
    "\n",
    "actions_array = np.array(actions_list, dtype=np.float32)\n",
    "print(f\"Actions shape: {actions_array.shape}\")\n",
    "\n",
    "# Create time array\n",
    "times = np.arange(len(actions_array)) / REPLAY_FREQUENCY_HZ\n",
    "\n",
    "# Get action names\n",
    "action_names = cfg.output_features[ACTION]\n",
    "D = actions_array.shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(min(D, 10), 1, figsize=(14, 2 * min(D, 10)), sharex=True)\n",
    "if D == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for d in range(min(D, 10)):\n",
    "    ax = axes[d]\n",
    "    ax.plot(times, actions_array[:, d], label=\"Ground Truth\", alpha=0.8)\n",
    "    ax.set_ylabel(action_names[d] if d < len(action_names) else f\"dim {d}\", fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_title(f\"Episode {EPISODE_INDEX}: Ground Truth Actions\")\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Episode duration: {times[-1]:.1f} seconds\")\n",
    "print(f\"Total actions: {len(actions_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ab31a",
   "metadata": {},
   "source": [
    "## 4. Connect to Robot\n",
    "\n",
    "Establish connection to the robot service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from example_policies.robot_deploy.robot_io.robot_service import robot_service_pb2_grpc\n",
    "from example_policies.robot_deploy.robot_io.robot_interface import RobotInterface\n",
    "from example_policies.robot_deploy.robot_io.robot_client import RobotClient\n",
    "from example_policies.robot_deploy.utils import print_info\n",
    "\n",
    "# Connect to robot\n",
    "print(f\"Connecting to robot at {SERVER_ENDPOINT}...\")\n",
    "channel = grpc.insecure_channel(SERVER_ENDPOINT)\n",
    "stub = robot_service_pb2_grpc.RobotServiceStub(channel)\n",
    "\n",
    "# Create robot interface\n",
    "robot_interface = RobotInterface(stub, cfg)\n",
    "\n",
    "# Create debug printer\n",
    "dbg_printer = print_info.InfoPrinter(cfg)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    obs = robot_interface.get_observation(device)\n",
    "    if obs:\n",
    "        print(\"‚úÖ Connected to robot!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Connected but no observation received\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075959f3",
   "metadata": {},
   "source": [
    "## 5. (Optional) Move Robot to Home Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move robot to home position before replay\n",
    "try:\n",
    "    response = robot_interface.move_home()\n",
    "    print(f\"‚úÖ Robot homing command sent\")\n",
    "    print(f\"   Response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Homing failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47007fd",
   "metadata": {},
   "source": [
    "## 6. Debug: Compare Dataset vs Live Observations\n",
    "\n",
    "Compare the format of observations from the dataset vs live robot to identify mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05931519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OBSERVATION (from parquet + video)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# State from parquet (use the df already loaded in cell 3)\n",
    "row = df.iloc[0]\n",
    "\n",
    "state_cols = sorted([c for c in df.columns if c.startswith(\"observation.state\")])\n",
    "state_values = []\n",
    "for c in state_cols:\n",
    "    val = row[c]\n",
    "    if isinstance(val, np.ndarray):\n",
    "        state_values.extend(val.flatten().tolist())\n",
    "    else:\n",
    "        state_values.append(float(val))\n",
    "dataset_state = torch.tensor(state_values, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"\\nobservation.state:\")\n",
    "print(f\"  Shape: {dataset_state.shape}\")\n",
    "print(f\"  Dtype: {dataset_state.dtype}\")\n",
    "print(f\"  Range: [{dataset_state.min():.4f}, {dataset_state.max():.4f}]\")\n",
    "print(f\"  First 5 values: {dataset_state[0, :5].tolist()}\")\n",
    "\n",
    "# Find available video keys by checking what exists\n",
    "video_dir = DATASET_DIR / \"videos\" / \"chunk-000\"\n",
    "video_keys = []\n",
    "dataset_images = {}\n",
    "\n",
    "if video_dir.exists():\n",
    "    for subdir in video_dir.iterdir():\n",
    "        if subdir.is_dir() and subdir.name.startswith(\"observation.images\"):\n",
    "            key = subdir.name\n",
    "            video_path = subdir / f\"episode_{EPISODE_INDEX:06d}.mp4\"\n",
    "            if video_path.exists():\n",
    "                video_keys.append(key)\n",
    "                container = av.open(str(video_path))\n",
    "                frame = next(container.decode(video=0))\n",
    "                img = frame.to_ndarray(format=\"rgb24\")\n",
    "                img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "                dataset_images[key] = img_tensor.unsqueeze(0).to(device)\n",
    "                container.close()\n",
    "                \n",
    "                print(f\"\\n{key}:\")\n",
    "                print(f\"  Shape: {dataset_images[key].shape}\")\n",
    "                print(f\"  Dtype: {dataset_images[key].dtype}\")\n",
    "                print(f\"  Range: [{dataset_images[key].min():.4f}, {dataset_images[key].max():.4f}]\")\n",
    "\n",
    "print(f\"\\nFound {len(video_keys)} video streams: {video_keys}\")\n",
    "\n",
    "# Get live observation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LIVE OBSERVATION (from robot)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "live_obs = robot_interface.get_observation(device)\n",
    "\n",
    "if live_obs:\n",
    "    # Check state\n",
    "    if \"observation.state\" in live_obs:\n",
    "        live_state = live_obs[\"observation.state\"]\n",
    "        print(f\"\\nobservation.state:\")\n",
    "        print(f\"  Shape: {live_state.shape}\")\n",
    "        print(f\"  Dtype: {live_state.dtype}\")\n",
    "        print(f\"  Range: [{live_state.min():.4f}, {live_state.max():.4f}]\")\n",
    "        print(f\"  First 5 values: {live_state[0, :5].tolist()}\")\n",
    "    \n",
    "    # Check images\n",
    "    for key in video_keys:\n",
    "        if key in live_obs:\n",
    "            live_img = live_obs[key]\n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"  Shape: {live_img.shape}\")\n",
    "            print(f\"  Dtype: {live_img.dtype}\")\n",
    "            print(f\"  Range: [{live_img.min():.4f}, {live_img.max():.4f}]\")\n",
    "    \n",
    "    print(f\"\\n\\nAll live obs keys: {list(live_obs.keys())}\")\n",
    "else:\n",
    "    print(\"‚ùå No observation received from robot\")\n",
    "\n",
    "# Visual comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUAL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if live_obs and video_keys:\n",
    "    n_images = len(video_keys)\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(5 * n_images, 10))\n",
    "    if n_images == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i, key in enumerate(video_keys):\n",
    "        # Dataset image\n",
    "        if key in dataset_images:\n",
    "            img_np = dataset_images[key][0].cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            axes[0, i].imshow(img_np)\n",
    "            axes[0, i].set_title(f\"Dataset: {key.split('.')[-1]}\")\n",
    "            axes[0, i].axis('off')\n",
    "        \n",
    "        # Live image\n",
    "        if key in live_obs:\n",
    "            img_np = live_obs[key][0].cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            axes[1, i].imshow(img_np)\n",
    "            axes[1, i].set_title(f\"Live: {key.split('.')[-1]}\")\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36db17",
   "metadata": {},
   "source": [
    "## 7. Move Robot to Start Position\n",
    "\n",
    "Move the robot to the starting position of the episode (first frame's TCP pose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first batch for starting position\n",
    "iterator = iter(dataloader)\n",
    "first_batch = next(iterator)\n",
    "\n",
    "if action_translator.action_mode in (ActionMode.DELTA_TCP, ActionMode.ABS_TCP, ActionMode.TCP):\n",
    "    state = first_batch[\"observation.state\"]\n",
    "    tcp_state = cfg.get_tcp_from_state(state[0].cpu().numpy().reshape(1, -1))\n",
    "    \n",
    "    # The robot expects the action to include gripper state as the last two elements\n",
    "    DEFAULT_GRIPPER_STATE = [0, 0]  # [left_gripper, right_gripper]\n",
    "    start_action = np.concatenate([tcp_state.flatten(), DEFAULT_GRIPPER_STATE]).astype(np.float32)\n",
    "    start_action = start_action[None, :]  # Add batch axis\n",
    "    \n",
    "    print(f\"Start TCP position (first 7 = left arm pos+quat):\")\n",
    "    print(f\"  {start_action[0, :7]}\")\n",
    "    \n",
    "    confirm = input(\"\\nPress Enter to move robot to start position (or 'skip' to skip): \")\n",
    "    if confirm.lower() != 'skip':\n",
    "        print(\"Moving robot to start position...\")\n",
    "        robot_interface.send_action(torch.from_numpy(start_action), ActionMode.ABS_TCP)\n",
    "        print(\"‚úÖ Robot moved to start position\")\n",
    "    else:\n",
    "        print(\"Skipped moving to start position\")\n",
    "else:\n",
    "    print(f\"Action mode {action_translator.action_mode} - skipping move to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53a024",
   "metadata": {},
   "source": [
    "## 8. Replay Episode to Robot\n",
    "\n",
    "‚ö†Ô∏è **This will move the robot!** Make sure the workspace is clear.\n",
    "\n",
    "This sends the ground truth actions from the recorded episode to the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec915e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Controller mode - CART_WAYPOINT is most stable\n",
    "CONTROLLER = RobotClient.CART_WAYPOINT\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  ROBOT REPLAY FROM DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Episode: {EPISODE_INDEX}\")\n",
    "print(f\"Frames: {len(actions_array)}\")\n",
    "print(f\"Frequency: {REPLAY_FREQUENCY_HZ} Hz\")\n",
    "print(f\"Speed: {SPEED_MULTIPLIER}x\")\n",
    "print(f\"Duration: {len(actions_array) / REPLAY_FREQUENCY_HZ / SPEED_MULTIPLIER:.1f} seconds\")\n",
    "print(f\"Action mode: {action_translator.action_mode}\")\n",
    "print(f\"Controller: {CONTROLLER}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "confirm = input(\"\\nType 'yes' to start replay: \")\n",
    "if confirm.lower() != 'yes':\n",
    "    print(\"Replay cancelled.\")\n",
    "else:\n",
    "    print(\"\\nüöÄ Starting replay...\")\n",
    "    \n",
    "    # Adjusted period based on speed multiplier\n",
    "    period = 1.0 / (REPLAY_FREQUENCY_HZ * SPEED_MULTIPLIER)\n",
    "    \n",
    "    try:\n",
    "        for step, action in enumerate(actions_array):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get current observation from robot (for action translation)\n",
    "            observation = robot_interface.get_observation(device)\n",
    "            \n",
    "            if observation:\n",
    "                # Convert action to tensor\n",
    "                action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "                # Translate action using the same translator as deployment\n",
    "                translated_action = action_translator.translate(action_tensor, observation)\n",
    "                \n",
    "                # Print debug info every 10 steps\n",
    "                if step % 10 == 0:\n",
    "                    dbg_printer.print(step, observation, translated_action, raw_action=False)\n",
    "                \n",
    "                # Send to robot\n",
    "                robot_interface.send_action(\n",
    "                    translated_action,\n",
    "                    action_translator.action_mode,\n",
    "                    CONTROLLER\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è No observation at step {step}\")\n",
    "            \n",
    "            # Wait for next cycle\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0.0, period - elapsed)\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Replay complete! Sent {len(actions_array)} actions.\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Replay interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during replay: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f916b",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the gRPC channel\n",
    "channel.close()\n",
    "print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
